{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('aps_failure_training_set.csv', na_values='na')\n",
    "test =  pd.read_csv('aps_failure_test_set.csv', na_values='na')\n",
    "\n",
    "train_labels = train['class']\n",
    "test_labels = test['class']\n",
    "train_features = train.drop('class', axis=1)\n",
    "test_features = test.drop('class', axis=1)\n",
    "\n",
    "train_labels = train_labels.replace({'neg':0, 'pos' : 1})\n",
    "test_labels = test_labels.replace({'neg':0, 'pos' : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictors</th>\n",
       "      <th>Missing_Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>br_000</td>\n",
       "      <td>82.106667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bq_000</td>\n",
       "      <td>81.203333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bp_000</td>\n",
       "      <td>79.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bo_000</td>\n",
       "      <td>77.221667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cr_000</td>\n",
       "      <td>77.215000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Predictors  Missing_Percentage\n",
       "0     br_000           82.106667\n",
       "1     bq_000           81.203333\n",
       "2     bp_000           79.566667\n",
       "3     bo_000           77.221667\n",
       "4     cr_000           77.215000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing Value Analysis\n",
    "miss_val = pd.DataFrame(train_features.isnull().sum())\n",
    "miss_val = miss_val.reset_index()\n",
    "miss_val = miss_val.rename(columns = {'index': 'Predictors', 0: 'Missing_Percentage'})\n",
    "miss_val['Missing_Percentage'] = (miss_val['Missing_Percentage']/len(train_features))*100\n",
    "miss_val = miss_val.sort_values('Missing_Percentage', ascending = False).reset_index(drop = True)\n",
    "miss_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Columns that have more than 35% missing values\n",
    "dropped = list(miss_val.loc[miss_val['Missing_Percentage'] > 35,'Predictors'])\n",
    "train_features.drop(columns = dropped, inplace = True)\n",
    "test_features.drop(columns = dropped, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_features)\n",
    "train_features = pd.DataFrame(scaler.transform(train_features), columns=train_features.columns)\n",
    "test_features = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\utils\\deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Imputing Missing Values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'median')\n",
    "imputer.fit(train_features)\n",
    "train_features = pd.DataFrame(imputer.transform(train_features), columns=train_features.columns)\n",
    "test_features = pd.DataFrame(imputer.transform(test_features), columns=test_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction - using Principal Component Analysis\n",
    "pca = PCA(0.99) #Define how much variance to explain\n",
    "pca.fit(train_features)\n",
    "best_train_features = pca.transform(train_features)\n",
    "best_train_features = pd.DataFrame(best_train_features)\n",
    "best_test_features = pd.DataFrame(pca.transform(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components 49\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of components {pca.n_components_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction using SelectKBest -- chosen over PCA\n",
    "k_best = 80\n",
    "selectKBest = SelectKBest(chi2, k_best)\n",
    "selectKBest.fit(train_features, train_labels)\n",
    "best_train_features = selectKBest.transform(train_features)\n",
    "idxs_selected = selectKBest.get_support(indices=True)\n",
    "best_train_features = train_features.iloc[:,idxs_selected]\n",
    "best_test_features = test_features.iloc[:,idxs_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion balanced: 2.5/1\n"
     ]
    }
   ],
   "source": [
    "#Balancing the dataset\n",
    "number_samples = 2500\n",
    "idxs_pos = train_labels[train_labels==1].index\n",
    "idxs_neg = train_labels[train_labels==0].sample(n=number_samples, replace=False, random_state=0).index\n",
    "idxs_balanced = np.concatenate((idxs_pos,idxs_neg))\n",
    "best_train_features_balanced = best_train_features.loc[idxs_balanced]\n",
    "train_labels_balanced = train_labels.loc[idxs_balanced]\n",
    "print(f'Proportion balanced: {number_samples/1000}/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   53.3s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=3)]: Done 270 out of 270 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='deviance', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=100,\n",
       "                                                  n_iter_no...\n",
       "                                                  presort='auto',\n",
       "                                                  random_state=0, subsample=1.0,\n",
       "                                                  tol=0.0001,\n",
       "                                                  validation_fraction=0.1,\n",
       "                                                  verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=3,\n",
       "             param_grid=[{'loss': ['deviance', 'exponential'],\n",
       "                          'min_samples_leaf': [5, 10, 50],\n",
       "                          'min_samples_split': [2, 100, 500],\n",
       "                          'n_estimators': range(50, 71, 10)}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Classification using Gradient Boosting methods\n",
    "gbc = GradientBoostingClassifier(random_state=0)\n",
    "params = [{'loss': ['deviance', 'exponential'] , 'n_estimators': range(50,71,10) , 'min_samples_split':[2,100,500]\n",
    "           ,'min_samples_leaf': [5,10,50]}]\n",
    "gbcc = GridSearchCV(gbc, params, cv=5, scoring='recall', verbose=10, n_jobs=3)\n",
    "gbcc.fit(best_train_features_balanced, train_labels_balanced)\n",
    "gbc = gbcc\n",
    "display(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     15625\n",
      "           1       0.42      0.96      0.59       375\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.71      0.96      0.79     16000\n",
      "weighted avg       0.99      0.97      0.97     16000\n",
      "\n",
      "Total cost is: 12400.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance of the Gradient Boosting algorithm and obtaining the cost\n",
    "y_pred = gbc.predict(best_test_features)\n",
    "report = classification_report(test_labels, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(test_labels, y_pred).ravel()\n",
    "cm = pd.DataFrame(cm.reshape((1,4)), columns=['tn', 'fp', 'fn', 'tp'])\n",
    "total_cost = 10*cm.fp + 500*cm.fn\n",
    "print(f'Total cost is: {float(total_cost.values[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:  9.0min finished\n"
     ]
    }
   ],
   "source": [
    "#Classificating using Random Forest\n",
    "params = [{'criterion': ['gini', 'entropy'], 'max_features': ['sqrt', 'log2'], 'n_estimators': range(100,201,25),\n",
    "           'max_depth': [10,20,30]}]\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "#Executa grid search com cross validation\n",
    "rfcc = GridSearchCV(rfc, params, cv=5, scoring='recall', verbose=10, n_jobs=3)\n",
    "rfcc.fit(best_train_features_balanced, train_labels_balanced)\n",
    "rfc = rfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     15625\n",
      "           1       0.42      0.97      0.59       375\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.71      0.97      0.79     16000\n",
      "weighted avg       0.99      0.97      0.97     16000\n",
      "\n",
      "Total cost is: 10460.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the Random Forest Model\n",
    "y_pred = rfc.predict(best_test_features)\n",
    "report = classification_report(test_labels, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(test_labels, y_pred).ravel()\n",
    "cm = pd.DataFrame(cm.reshape((1,4)), columns=['tn', 'fp', 'fn', 'tp'])\n",
    "total_cost = 10*cm.fp + 500*cm.fn\n",
    "print(f'Total cost is: {float(total_cost.values[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:   56.5s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=3,\n",
       "             param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10],\n",
       "                          'gamma': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
       "                          'kernel': ['rbf', 'linear']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification using Support Vector Machines\n",
    "params = [{'kernel': ['rbf', 'linear'], 'gamma': [0.01,0.05, 0.1, 0.2,0.3,0.4], 'C': [0.001, 0.01, 0.1, 1,10]}]\n",
    "svmc = GridSearchCV(svm.SVC(C=1), params, cv=5, scoring='recall', verbose=10, n_jobs=3)\n",
    "svmc.fit(best_train_features_balanced, train_labels_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     15625\n",
      "           1       0.48      0.92      0.63       375\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.74      0.95      0.81     16000\n",
      "weighted avg       0.99      0.97      0.98     16000\n",
      "\n",
      "Total cost is: 18720.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the SVM model\n",
    "y_pred = svmc.predict(best_test_features)\n",
    "report = classification_report(test_labels, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(test_labels, y_pred).ravel()\n",
    "cm = pd.DataFrame(cm.reshape((1,4)), columns=['tn', 'fp', 'fn', 'tp'])\n",
    "total_cost = 10*cm.fp + 500*cm.fn\n",
    "print(f'Total cost is: {float(total_cost.values[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Neural Network\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 80))\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3500/3500 [==============================] - 3s 853us/step - loss: 0.5155 - acc: 0.8654\n",
      "Epoch 2/10\n",
      "3500/3500 [==============================] - 1s 143us/step - loss: 0.1920 - acc: 0.9437\n",
      "Epoch 3/10\n",
      "3500/3500 [==============================] - 0s 126us/step - loss: 0.1611 - acc: 0.9469\n",
      "Epoch 4/10\n",
      "3500/3500 [==============================] - 0s 134us/step - loss: 0.1552 - acc: 0.9474\n",
      "Epoch 5/10\n",
      "3500/3500 [==============================] - 0s 138us/step - loss: 0.1518 - acc: 0.9483\n",
      "Epoch 6/10\n",
      "3500/3500 [==============================] - 1s 150us/step - loss: 0.1493 - acc: 0.9480\n",
      "Epoch 7/10\n",
      "3500/3500 [==============================] - 0s 141us/step - loss: 0.1481 - acc: 0.9483\n",
      "Epoch 8/10\n",
      "3500/3500 [==============================] - 1s 150us/step - loss: 0.1464 - acc: 0.9497\n",
      "Epoch 9/10\n",
      "3500/3500 [==============================] - 1s 154us/step - loss: 0.1448 - acc: 0.9506\n",
      "Epoch 10/10\n",
      "3500/3500 [==============================] - 1s 143us/step - loss: 0.1432 - acc: 0.9486\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(best_train_features_balanced, train_labels_balanced, batch_size = 10, epochs = 10)\n",
    "y_pred = classifier.predict(best_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     15625\n",
      "           1       0.36      0.96      0.52       375\n",
      "\n",
      "    accuracy                           0.96     16000\n",
      "   macro avg       0.68      0.96      0.75     16000\n",
      "weighted avg       0.98      0.96      0.97     16000\n",
      "\n",
      "Total cost is: 14390.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the neural network\n",
    "y_pred = (y_pred > 0.5)\n",
    "report = classification_report(test_labels, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(test_labels, y_pred).ravel()\n",
    "cm = pd.DataFrame(cm.reshape((1,4)), columns=['tn', 'fp', 'fn', 'tp'])\n",
    "total_cost = 10*cm.fp + 500*cm.fn\n",
    "print(f'Total cost is: {float(total_cost.values[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=3)]: Done 135 out of 135 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=3,\n",
       "             param_grid=[{'learning_rate': [0.01, 0.05, 0.1],\n",
       "                          'max_depth': [3, 4, 5],\n",
       "                          'n_estimators': [50, 100, 150]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification using XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "params = [{'max_depth': [3,4,5], 'n_estimators': [50,100,150], 'learning_rate': [ 0.01,0.05,0.1]}]\n",
    "xgb = GridSearchCV(XGBClassifier(), params, cv=5, scoring='recall', verbose=10, n_jobs=3)\n",
    "xgb.fit(best_train_features_balanced, train_labels_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     15625\n",
      "           1       0.49      0.97      0.65       375\n",
      "\n",
      "    accuracy                           0.98     16000\n",
      "   macro avg       0.75      0.97      0.82     16000\n",
      "weighted avg       0.99      0.98      0.98     16000\n",
      "\n",
      "Total cost is: 10230.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluating \n",
    "y_pred = xgb.predict(best_test_features)\n",
    "report = classification_report(test_labels, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(test_labels, y_pred).ravel()\n",
    "cm = pd.DataFrame(cm.reshape((1,4)), columns=['tn', 'fp', 'fn', 'tp'])\n",
    "total_cost = 10*cm.fp + 500*cm.fn\n",
    "print(f'Total cost is: {float(total_cost.values[0])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
